1. Finish training MMNASNET.

2. Test the MMNASNET on VQAv2 validation set and get overall accuracy 67.65%

3. Test the MMNASNET on VQAv2 test set and get overall accuracy 69.56%

4. Modified the Guided Attention (GA) Unit. Compute the key and value from image and query from question. 
   Since the dimension does not match, add a fully connection layer after it.
   Modified files are named after "ModiMMNAS".

5. Add self.lstm.flatten_parameters() to the net.py to avoid constant warning.

6. The modified network is using the same structure of SA, FFN and RSA unit. GA unit is changed.
   GA(x,y,x_mask)
   The key and value is computed by x (image)
   The query is computed by y (question)
   The mask is derived from x (image)
   After a single transformer layer, the output will be connected via a fully connected layer and an activate layer to maintain the dimension.


